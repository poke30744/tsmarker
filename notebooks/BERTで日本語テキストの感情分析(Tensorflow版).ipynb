{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c9efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2daa8ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['猫', 'が', 'かわい', 'です']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_nameはここから取得(cf. https://huggingface.co/transformers/pretrained_models.html)\n",
    "#model_name = \"cl-tohoku/bert-base-japanese\"\n",
    "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = transformers.BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.tokenize('猫がかわいです')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ecfc7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1106173   ['input_1[0][0]',             \n",
      " )                           ngAndCrossAttentions(last_   44         'input_2[0][0]',             \n",
      "                             hidden_state=(None, 128, 7              'input_3[0][0]']             \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    1538      ['tf_bert_model[0][1]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110618882 (421.98 MB)\n",
      "Trainable params: 1538 (6.01 KB)\n",
      "Non-trainable params: 110617344 (421.97 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(model_name, num_classes, max_length):\n",
    "    input_shape = (max_length, )\n",
    "    input_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "    token_type_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(model_name)\n",
    "    bert_model.trainable = False\n",
    "    base_model_output = bert_model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    )\n",
    "    last_hidden_state, pooler_output = base_model_output.last_hidden_state, base_model_output.pooler_output\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(pooler_output)\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=[output])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    return model\n",
    "\n",
    "num_classes = 2\n",
    "max_length = 128\n",
    "model = build_model(model_name, num_classes=num_classes, max_length=max_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8ac8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストのリストをtransformers用の入力データに変換\n",
    "def to_features(texts, max_length):\n",
    "    shape = (len(texts), max_length)\n",
    "    # input_idsやattention_mask, token_type_idsの説明はglossaryに記載(cf. https://huggingface.co/transformers/glossary.html)\n",
    "    input_ids = np.zeros(shape, dtype=\"int32\")\n",
    "    attention_mask = np.zeros(shape, dtype=\"int32\")\n",
    "    token_type_ids = np.zeros(shape, dtype=\"int32\")\n",
    "    for i, text in enumerate(texts):\n",
    "        encoded_dict = tokenizer.encode_plus(text, max_length=max_length, pad_to_max_length=True)\n",
    "        input_ids[i] = encoded_dict[\"input_ids\"]\n",
    "        attention_mask[i] = encoded_dict[\"attention_mask\"]\n",
    "        token_type_ids[i] = encoded_dict[\"token_type_ids\"]\n",
    "    return [input_ids, attention_mask, token_type_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d673b448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['♬～ ≪(観客の拍手) (女の子Ａ･女の子Ｂ)久しぶり～！ (女の子Ａ)大丈夫｡ (女の子Ｂ)うん｡ (男性)ただいま｡ (女性)おかえり｡ (母)ありがとうございます｡',\n",
       "  'また むずい… 難しい｡ (林田)ただいま！ えっ？ ああ お… おかえり…｡ (階段を上がる音) えっ？ ♬～',\n",
       "  '(店主)いらっしゃい かがわ (香川)じゃあ 上ロース 卵つけて さかい (堺)と ｢パーフェクトサントリービール｣ ２つ ピーエスビー はい ＰＳＢ ２丁!! からの～ 乾杯したい ですよね う! う! うまい!うまい! うまい!! <ＰＳＢ ｢パーフェクト サントリービール｣> 圧巻 どぇす!',\n",
       "  '(ﾊﾞｰﾙ)《キリヲ》 (ｷﾘｦ)フッ',\n",
       "  '♬～ みやざわ やまだ (宮沢)＜がんばった一年だもの｡＞ (山田)あ ＜きっと今 日本中が 同じ気持ちです｡＞ かわぐち (川口)あ おぐり (友人)あ 雪！ (小栗)おっ ごほうびごほうび～♪ 結構がんばってんだよね (父親)みたいだな 見てるぞ～'],\n",
       " [1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def LoadDataset(path: Path):\n",
    "    with path.open('r') as f:\n",
    "        dataset = json.load(f)\n",
    "    cm = dataset['cm']\n",
    "    noncm = dataset['noncm']\n",
    "    random.shuffle(cm)\n",
    "    random.shuffle(noncm)\n",
    "    # drop some non-cm clips\n",
    "    noncm = noncm[:len(cm)]\n",
    "\n",
    "    texts, labels = cm + noncm, [1] * len(cm) + [0] * len(cm)\n",
    "    return shuffle(texts, labels)\n",
    "\n",
    "texts, labels = LoadDataset(Path(r'..\\speech.json'))\n",
    "\n",
    "texts[:5], labels[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18c767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\repos\\tsmarker\\bert\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_train = to_features(texts, max_length)\n",
    "y_train = tf.keras.utils.to_categorical(labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac9640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "26/26 [==============================] - 734s 28s/step - loss: 0.7408 - acc: 0.4409\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 812s 31s/step - loss: 0.7291 - acc: 0.4608\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 983s 38s/step - loss: 0.7183 - acc: 0.4885\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 1043s 40s/step - loss: 0.7102 - acc: 0.5072\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 1179s 46s/step - loss: 0.7009 - acc: 0.5362\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 849s 32s/step - loss: 0.6967 - acc: 0.5326\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 756s 29s/step - loss: 0.6851 - acc: 0.5609\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 761s 29s/step - loss: 0.6838 - acc: 0.5712\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 756s 29s/step - loss: 0.6769 - acc: 0.5760\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 777s 30s/step - loss: 0.6715 - acc: 0.6037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a600690580>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eac6c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "26/26 [==============================] - 719s 28s/step - loss: 0.6602 - acc: 0.6230\n",
      "Epoch 2/20\n",
      "26/26 [==============================] - 745s 29s/step - loss: 0.6598 - acc: 0.6255\n",
      "Epoch 3/20\n",
      "26/26 [==============================] - 746s 29s/step - loss: 0.6553 - acc: 0.6297\n",
      "Epoch 4/20\n",
      "26/26 [==============================] - 757s 29s/step - loss: 0.6467 - acc: 0.6429\n",
      "Epoch 5/20\n",
      "26/26 [==============================] - 751s 29s/step - loss: 0.6488 - acc: 0.6255\n",
      "Epoch 6/20\n",
      "26/26 [==============================] - 749s 29s/step - loss: 0.6450 - acc: 0.6429\n",
      "Epoch 7/20\n",
      "26/26 [==============================] - 747s 29s/step - loss: 0.6388 - acc: 0.6429\n",
      "Epoch 8/20\n",
      "26/26 [==============================] - 751s 29s/step - loss: 0.6292 - acc: 0.6634\n",
      "Epoch 9/20\n",
      "26/26 [==============================] - 759s 29s/step - loss: 0.6261 - acc: 0.6659\n",
      "Epoch 10/20\n",
      "26/26 [==============================] - 758s 29s/step - loss: 0.6180 - acc: 0.6797\n",
      "Epoch 11/20\n",
      "26/26 [==============================] - 760s 29s/step - loss: 0.6163 - acc: 0.6834\n",
      "Epoch 12/20\n",
      "26/26 [==============================] - 759s 29s/step - loss: 0.6192 - acc: 0.6852\n",
      "Epoch 13/20\n",
      "26/26 [==============================] - 752s 29s/step - loss: 0.6056 - acc: 0.6948\n",
      "Epoch 14/20\n",
      "26/26 [==============================] - 758s 29s/step - loss: 0.6114 - acc: 0.6828\n",
      "Epoch 15/20\n",
      "26/26 [==============================] - 761s 29s/step - loss: 0.5996 - acc: 0.7014\n",
      "Epoch 16/20\n",
      "26/26 [==============================] - 758s 29s/step - loss: 0.6032 - acc: 0.6882\n",
      "Epoch 17/20\n",
      "26/26 [==============================] - 754s 29s/step - loss: 0.6029 - acc: 0.6990\n",
      "Epoch 18/20\n",
      "26/26 [==============================] - 761s 29s/step - loss: 0.5929 - acc: 0.7075\n",
      "Epoch 19/20\n",
      "26/26 [==============================] - 764s 29s/step - loss: 0.5908 - acc: 0.7033\n",
      "Epoch 20/20\n",
      "26/26 [==============================] - 758s 29s/step - loss: 0.5967 - acc: 0.6918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a60ba4ecd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e9eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc0f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e64cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed703c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca8673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "29c924f6",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# model_nameはここから取得(cf. https://huggingface.co/transformers/pretrained_models.html)\n",
    "#model_name = \"cl-tohoku/bert-base-japanese\"\n",
    "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = transformers.BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 訓練データ\n",
    "train_texts = [\n",
    "    \"この犬は可愛いです\",\n",
    "    \"その猫は気まぐれです\",\n",
    "    \"あの蛇は苦手です\"\n",
    "]\n",
    "train_labels = [1, 0, 0] # 1: 好き, 0: 嫌い\n",
    "\n",
    "# テストデータ\n",
    "test_texts = [\n",
    "    \"その猫はかわいいです\",\n",
    "    \"どの鳥も嫌いです\",\n",
    "    \"あのヤギは怖いです\"\n",
    "]\n",
    "test_labels = [1, 0, 0]\n",
    "\n",
    "# テキストのリストをtransformers用の入力データに変換\n",
    "def to_features(texts, max_length):\n",
    "    shape = (len(texts), max_length)\n",
    "    # input_idsやattention_mask, token_type_idsの説明はglossaryに記載(cf. https://huggingface.co/transformers/glossary.html)\n",
    "    input_ids = np.zeros(shape, dtype=\"int32\")\n",
    "    attention_mask = np.zeros(shape, dtype=\"int32\")\n",
    "    token_type_ids = np.zeros(shape, dtype=\"int32\")\n",
    "    for i, text in enumerate(texts):\n",
    "        encoded_dict = tokenizer.encode_plus(text, max_length=max_length, pad_to_max_length=True)\n",
    "        input_ids[i] = encoded_dict[\"input_ids\"]\n",
    "        attention_mask[i] = encoded_dict[\"attention_mask\"]\n",
    "        token_type_ids[i] = encoded_dict[\"token_type_ids\"]\n",
    "    return [input_ids, attention_mask, token_type_ids]\n",
    "\n",
    "# 単一テキストをクラス分類するモデルの構築\n",
    "def build_model(model_name, num_classes, max_length):\n",
    "    input_shape = (max_length, )\n",
    "    input_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "    token_type_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(model_name)\n",
    "    base_model_output = bert_model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    )\n",
    "    last_hidden_state, pooler_output = base_model_output.last_hidden_state, base_model_output.pooler_output\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(pooler_output)\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=[output])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    return model\n",
    "\n",
    "num_classes = 2\n",
    "max_length = 15\n",
    "batch_size = 10\n",
    "epochs = 3\n",
    "\n",
    "x_train = to_features(train_texts, max_length)\n",
    "y_train = tf.keras.utils.to_categorical(train_labels, num_classes=num_classes)\n",
    "model = build_model(model_name, num_classes=num_classes, max_length=max_length)\n",
    "\n",
    "# 訓練\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# 予測\n",
    "x_test = to_features(test_texts, max_length)\n",
    "y_test = np.asarray(test_labels)\n",
    "y_preda = model.predict(x_test)\n",
    "y_pred = np.argmax(y_preda, axis=1)\n",
    "print(\"Accuracy: %.5f\" % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a92a777d",
   "metadata": {},
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5959ceae",
   "metadata": {},
   "source": [
    "model.layers[2].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71dacac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
